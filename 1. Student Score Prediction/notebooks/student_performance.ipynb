# **1. Importing Dataset**
import kagglehub
import shutil

# Download latest version
path = kagglehub.dataset_download("rabieelkharoua/students-performance-dataset")
print("Path to dataset files:", path)

# Changing the path of Dataset
src_path = "/root/.cache/kagglehub/datasets/rabieelkharoua/students-performance-dataset/versions/2"
dst_path = "/content/students-performance-dataset"

# Copy dataset folder to /content
shutil.copytree(src_path, dst_path, dirs_exist_ok=True)
print("Dataset copied to:", dst_path)


# **2. Loading Dataset**
import pandas as pd

# Load dataset
df = pd.read_csv("/content/students-performance-dataset/Student_performance_data _.csv")

# Show first rows
df.head()


# **3. Data Cleaning**
# Dataset info
print(df.info())

# Checking missing values
print(df.isnull().sum())


# **4. Data Visualization**
import matplotlib.pyplot as plt
import seaborn as sns

# Scatter Plot: StudyTimeWeekly vs GPA
plt.figure(figsize=(8,6))
sns.scatterplot(x="StudyTimeWeekly", y="GPA", data=df)
plt.title("Weekly Study Time vs GPA")
plt.xlabel("Study Time (Weekly)")
plt.ylabel("GPA")
plt.show()

# Correlation Heatmap
plt.figure(figsize=(10,6))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()


# **5. Data Splitting**
from sklearn.model_selection import train_test_split

# Features and target
X = df[['StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport']]
y = df['GPA']

# Split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape


# **6. Model Training**

from sklearn.linear_model import LinearRegression

# Initialize model
model = LinearRegression()

# Train model
model.fit(X_train, y_train)

# Coefficients and intercept
print("Intercept:", model.intercept_)
print("Coefficients:", model.coef_)


# **7. Model Evaluation**
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

# Predictions
y_pred = model.predict(X_test)

# R-squared
r2 = r2_score(y_test, y_pred)

# RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("R² Score:", r2)
print("RMSE:", rmse)


# **7. Visualization of Predictions**
plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
plt.plot([0, 4], [0, 4], color='red', linestyle='--')  # perfect prediction line
plt.xlabel("Actual GPA")
plt.ylabel("Predicted GPA")
plt.title("Actual vs Predicted GPA")
plt.show()


# **8. Polynomial Regression**

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# Create Polynomial Regression model (degree=2)
poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())

# Train the model
poly_model.fit(X_train, y_train)

# Predictions
y_poly_pred = poly_model.predict(X_test)

# Evaluation metrics
r2_poly = r2_score(y_test, y_poly_pred)
rmse_poly = np.sqrt(mean_squared_error(y_test, y_poly_pred))

print("Polynomial Regression (degree=2) R²:", r2_poly)
print("Polynomial Regression (degree=2) RMSE:", rmse_poly)


# **8.2 Feature Engineering Experiments**
# Use all features except StudentID, GPA, and GradeClass
X_full = df.drop(columns=['StudentID', 'GPA', 'GradeClass'])
y_full = df['GPA']

# Train-test split
X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(
    X_full, y_full, test_size=0.2, random_state=42
)

# Train linear regression on full features
model_full = LinearRegression()
model_full.fit(X_train_full, y_train_full)

# Predictions
y_full_pred = model_full.predict(X_test_full)

# Evaluation
r2_full = r2_score(y_test_full, y_full_pred)
rmse_full = np.sqrt(mean_squared_error(y_test_full, y_full_pred))

print("Full Feature Model R²:", r2_full)
print("Full Feature Model RMSE:", rmse_full)


# **9. Comparison and Conclusion**
### **Model Performance Summary**

| **Model**                           | **Features Used**                          | **R² Score** | **RMSE** |
|-------------------------------------|--------------------------------------------|--------------|----------|
| **Base Linear Regression**          | `StudyTimeWeekly, Absences, ParentalSupport, Tutoring` | 0.9283       | 0.2434   |
| **Polynomial Regression (Degree=2)**| Same 4 features                            | 0.9278       | 0.2444   |
| **Full Feature Model**              | All features except *StudentID* & target   | **0.9532**   | **0.1966** |

---

### **Key Insights**
1. **Linear Regression (base model)** already performed very well, explaining about *93% of the variance* in GPA.  
2. **Polynomial Regression** did **not** improve performance, meaning the relationship between GPA and features is mostly linear.  
3. The **Full Feature Model** performed the best, explaining *95.3% of the variance* with a lower RMSE of *0.19*.  
4. This shows GPA is not only influenced by study time and absences, but also by **extracurricular activities, parental education, and other lifestyle factors**.

---

### **Conclusion**
- A **simple linear regression** with a few features provides good accuracy, but it misses important context.  
- **Polynomial regression added complexity without benefit**, confirming that GPA relationships are largely linear.  
- The **full feature linear regression model** is the most effective, achieving **R² ≈ 0.95** and **RMSE ≈ 0.19**.  
- For future improvement, one could experiment with **advanced models** such as *Random Forest, Gradient Boosting, or Neural Networks* to see if they can capture more complex relationships.  

✅ **Final Takeaway:** GPA is best predicted when combining academic effort (*study time, absences*) with *family, extracurricular, and lifestyle factors*.  
